{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"MC2_NeitherCatNorDog.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zg1x1IceilXI"},"source":["# Preparations"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":2951,"status":"ok","timestamp":1560869720876,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"5378LoOjVAQh","outputId":"919bbc78-c7b8-4644-c547-778fffc79b3f","colab":{"base_uri":"https://localhost:8080/","height":157}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/drive')\n","np.random.seed(123)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CA9IJ7e5RvYE"},"source":["In this notebook, we will predict the star-rating of Amazon book reviews based on the textual data of the reviews. First, let´s load the data and have a look at it:"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":29511,"status":"ok","timestamp":1560768789527,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"8CWmVQ3QilXq","outputId":"9c076fe3-a836-494e-a7f1-be3c47a7fcf3","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["!cp \"/content/drive/My Drive/Colab Notebooks/data/MC2.csv\" \"MC2.csv\"\n","minidata = pd.read_csv(\"MC2.csv\")\n","minidata = minidata.drop('Unnamed: 0', axis=1)\n","minidata = minidata.dropna()\n","minidata.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_id</th>\n","      <th>product_title</th>\n","      <th>star_rating</th>\n","      <th>helpful_votes</th>\n","      <th>total_votes</th>\n","      <th>verified_purchase</th>\n","      <th>review_headline</th>\n","      <th>review_body</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0983797706</td>\n","      <td>Igniting Your True Purpose and Passion: A Busi...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>This is an inspirational and insightful book t...</td>\n","      <td>This is an inspirational and insightful book t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1423151283</td>\n","      <td>The Duckling Gets a Cookie!? (Pigeon)</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>Loved it!</td>\n","      <td>My twins are 3 and they love the pigeon books!...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1445604752</td>\n","      <td>Spitfire Ace of Aces: The Wartime Story of Joh...</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>N</td>\n","      <td>Engaging Account of the Combat Career of an RA...</td>\n","      <td>Back in 1964, I was introduced to 'Johnnie' Jo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0393057941</td>\n","      <td>The Bread Bible</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>Great book for beginners</td>\n","      <td>I disagree with those reviews that say this is...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1608322858</td>\n","      <td>Do It Well. Make It Fun.: The Key to Success i...</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>N</td>\n","      <td>Left me wanting more</td>\n","      <td>Reading this book, I hoped to find more about ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   product_id  ...                                        review_body\n","0  0983797706  ...  This is an inspirational and insightful book t...\n","1  1423151283  ...  My twins are 3 and they love the pigeon books!...\n","2  1445604752  ...  Back in 1964, I was introduced to 'Johnnie' Jo...\n","3  0393057941  ...  I disagree with those reviews that say this is...\n","4  1608322858  ...  Reading this book, I hoped to find more about ...\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Mwr-yri0Si7o"},"source":["Now, we need to extract the variables we are interested in, i.e. the the two variables containing the textual data (\"review_body\" and \"review_headline\") and the target variable \"star_rating\". We start with the variable \"star_rating\" by one-hot encoding it and storing it in \"y\". As one can easily see in the bar plot denoting the frequency of the different star ratings, the data is highly imbalanced. Around 61% of all ratings are five-star ratings."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":30427,"status":"ok","timestamp":1560768791990,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"7NNU1X5vhRS1","outputId":"56d51acb-1ade-42b4-b668-95123e22e01c","colab":{"base_uri":"https://localhost:8080/","height":314}},"source":["y = minidata.star_rating-1\n","y = to_categorical(y)\n","\n","plt.figure(figsize=(6,4))\n","sns.countplot(x=\"star_rating\", data=minidata)\n","plt.ylabel('Count', fontsize=12)\n","plt.xlabel('Ratings', fontsize=12)\n","plt.xticks(rotation='vertical')\n","plt.title(\"Ratings Frequency\", fontsize=15)\n","plt.show()\n","\n","i = 0\n","for num in range(y.shape[0]):\n","  if y[num][4] == max(y[num]):\n","    i = i+1\n","i = round((i/y.shape[0])*100)\n","print('%s percent of the data belong to the majority class (five-star review).' % i)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEYCAYAAABslZDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgwmAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["61 percent of the data belong to the majority class (five-star review).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BWpi2kwyT1N-"},"source":["After that, we move on to the next variable \"review_body\", to which we first add the textual data of \"review_headline\". The preprocessing for the textual data is somewhat more complex. As we can´t feed pure textual data to a neural network, we first need to transform each word to a number (which will be transformed to a vector via an embedding layer later on). To do this, we can use text preprocessing functions from both NLTK and Keras. First we tokenize the text, convert all letters to lowercase, remove numbers and stopwords, and use a pre-built lemmatization function. As we want to do sentiment analysis, however, we need to be careful what type of stop words we are excluding. The list of stop words from NLTK consists of words like \"not\" or \"don't\". Those type of words, however, are likely to be important for sentiment analysis, as they can totally change the sentiment of a sentence. Therefore, we create our own list of stopword based on the list of stopwords from NLTK, but excluding negating words.\n","\n","After this step, we move on to the Tokenizer function from Keras. As fitting the Keras Tokenizer on text quickly blows up the RAM,  one could alternatively only use the first 100,000 reviews and assume that the most frequent words in these reviews will be somewhat similar to the most frequent words in the entire text corpus. As we do not want the network to pick up every occuring word within the text corups, we furthermore limit the maximum number of words captured by the tokenizer to 5,000, which represents approximately the 1% of the most frequent words occuring in the texts. Of course, this means that way more than 1% of the words will considered by the network as we are using the 1% most frequent words). We can calculate the 1% by looking at the maximum number of words and the lenght of the word index (representing the number of all occuring words).\n","\n","After fitting the tokenizer, we can then transform each review into a sequence of numbers and pad the sequence to a length of 80 and transform the resulting sequences such that it has the right shape for the network and save them in the variable \"x\". We decided on a sequence lengh of 80 because when testing different values, lower values harmed the performance, while the increase of the model performance was small compared to the increase in training time needed for each epoch."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DLtvNltwElW6","colab":{}},"source":["minidata['review_body'] = minidata['review_headline'].str.cat(minidata['review_body'],sep=\" \")\n","x = minidata.review_body\n","\n","stop_words = ['i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",'your','yours','yourself','yourselves','he','him','his','himself','she',\"she's\",'her','hers','herself','it',\"it's\",'its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that',\"that'll\",'these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only','own','same','so','than','too','very','s','can','will','just','now','d','ll','m','o','re','ve','y','ma']\n","lemmatizer = WordNetLemmatizer()\n","def fun_nltk(txt):\n","  tok = nltk.word_tokenize(txt)\n","  tok = [word.lower() for word in tok if word.isalpha()]\n","  tok = [word for word in tok if word not in stop_words]\n","  tok = [lemmatizer.lemmatize(word, pos='n') for word in tok]\n","  return tok\n","x = x.apply(fun_nltk)\n","x = x.apply(' '.join)\n","\n","#x.to_pickle(\"/content/drive/My Drive/Colab Notebooks/x and y/xpre.pkl\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9XK42bFjnojJ","colab":{}},"source":["#0 - only for internal use to save time running the same code aganin\n","!cp \"/content/drive/My Drive/Colab Notebooks/x and y/xpre.pkl\" \"xpre.pkl\"\n","x = pd.read_pickle(\"xpre.pkl\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":280194,"status":"ok","timestamp":1560717545452,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"_nQNg2A0XFFH","outputId":"d5a86c40-7d8e-43a8-ae3c-d813b6e40ddf","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["MAX_NB_WORDS = 5000\n","text_for_tokenizer = ' '.join(x)\n","#RAM-intensive; alternative would be: text_for_tokenizer = ' '.join(x.iloc[:TEXT_LENGHT])\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts([text_for_tokenizer])\n","word_index = tokenizer.word_index\n","print(MAX_NB_WORDS/len(word_index))\n","def fun1(num):\n","  return tokenizer.texts_to_sequences([num])\n","x = x.apply(fun1)\n","MAX_SEQUENCE_LENGTH = 80\n","def fun2(num):\n","  return pad_sequences(num, maxlen=MAX_SEQUENCE_LENGTH)\n","x = x.apply(fun2)\n","x = np.stack(x, axis=0)\n","x = x.reshape((x.shape[0], MAX_SEQUENCE_LENGTH))\n","\n","#word_index.keys()\n","#word_index[\"word\"]\n","#np.save(\"/content/drive/My Drive/Colab Notebooks/x and y/x\", x)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.009955062846311749\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XUMIl_BpYxf6"},"source":["Now, after having defined our feature variable \"x\" and target variable \"y\", we the can create our training and test data:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Lh5XgJbrfnel","colab":{}},"source":["#1 - only for internal use to save time running the same code aganin\n","!cp \"/content/drive/My Drive/Colab Notebooks/x and y/x.npy\" \"x.npy\"\n","x = np.load(\"x.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RpmAzor7fnRh","colab":{}},"source":["#2 - only for internal use to save time running the same code aganin\n","!cp \"/content/drive/My Drive/Colab Notebooks/x and y/y.txt\" \"y.txt\"\n","y = np.loadtxt(\"y.txt\", delimiter=\",\",  dtype='int')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8chp4YFpVARB","colab":{}},"source":["#3\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-CBJwZW4ZM7b"},"source":["Before we start training our network, however, we perform one final step. As mentioned earlier, the integer-coded words are transformed to vectors by an embedding layer. Training such a word embedding can take much time and require a lot of data. Therefore, we tried out different pre-trained word embeddings. By setting the weights of the embedding matrix to be trainable, it is also possible to use these pre-trained word embeddings as starting points rather than fixed embeddings (see for more details in the training section).\n","\n","The first one was the GloVe word embedding from Stanford University, which was created by implementing a popular embedding technique based on matrix factorization of word co-occurence statistics using Wikipedia articles. More specifically, we implemented the 100-dimensional version of the GloVe embedding, meaning that each word is represented by a 100-dimensional vector. After importing the pretrained word embeddings, we create the embedding matrix (which will be fed to the embedding layer as its trainable/non-trainable weights later on) based on the imported embeddings and our tokenizer which we had fitted before. When inputting an one-hot-encoded word, the embedding layer outputs the respective 100-dimensional word vector from the embedding.\n","\n","As we are dealing with sentiment analysis, however, it might be a good idea to use a word embedding which were trained on product reviews. Therefore, we also implemented a 300-dimensional word embedding which was trained on Amazon reviews (see http://softeng.polito.it/erion/).\n","\n","Yet, using the pretrained GloVe from Stanford university turned out to produce better results than when using the trained embeddings from a similar domain. Terefore, we will use the GloVe word embedding.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"75l7sft3tIi8","colab":{}},"source":["!cp \"/content/drive/My Drive/Colab Notebooks/data/glove.6B.100d.txt\" \"glove.6B.100d.txt\"\n","f = open('glove.6B.100d.txt')\n","embeddings_index = {}\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","EMBEDDING_DIM = 100\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","#np.save(\"/content/drive/My Drive/Colab Notebooks/x and y/embedding_matrix\", embedding_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ATyIoKb4fz4J","colab":{}},"source":["#4 - only for internal use to save time running the same code aganin\n","!cp \"/content/drive/My Drive/Colab Notebooks/x and y/embedding_matrix.npy\" \"embedding_matrix.npy\"\n","embedding_matrix = np.load(\"embedding_matrix.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p7IL1y0cg1BE","colab":{}},"source":["#5 - only for internal use to save time running the same code aganin\n","MAX_SEQUENCE_LENGTH = 80\n","EMBEDDING_DIM = 100\n","#word_index = np.repeat(0, 502257)"],"execution_count":0,"outputs":[]}]}